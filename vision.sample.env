# This sample env file can be used to set environment variables for the docker-compose.yml
# Copy this file to vision.env and uncomment the model of your choice.
HF_HOME=hf_home
HF_HUB_ENABLE_HF_TRANSFER=1
#HF_TOKEN=hf-...
#CUDA_VISIBLE_DEVICES=1,0
#OPENEDAI_DEVICE_MAP="sequential"

#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-1B -A flash_attention_2"  # test ✅ pass, time: 5.7s, mem: 3.3GB, 13/13 tests passed, 29T/1.3790647983551025s, 21.0 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-2B -A flash_attention_2"  # test ✅ pass, time: 6.2s, mem: 5.6GB, 13/13 tests passed, 29T/1.5570149421691895s, 18.6 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-4B -A flash_attention_2"  # test ✅ pass, time: 8.3s, mem: 9.6GB, 13/13 tests passed, 32T/2.2523040771484375s, 14.2 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-8B -A flash_attention_2"  # test ✅ pass, time: 8.8s, mem: 17.7GB, 13/13 tests passed, 33T/2.4007771015167236s, 13.7 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-16B -A flash_attention_2"  # test ✅ pass, time: 12.3s, mem: 31.3GB, 13/13 tests passed, 32T/3.656616449356079s, 8.8 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis2-34B -A flash_attention_2"  # test ✅ pass, time: 21.2s, mem: 66.1GB, 13/13 tests passed, 32T/6.304844617843628s, 5.1 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis1.6-Llama3.2-3B -A flash_attention_2"  # test ✅ pass, time: 13.5s, mem: 10.8GB, 13/13 tests passed, 125T/3.97086763381958s, 31.5 T/s
#CLI_COMMAND="python vision.py -m AIDC-AI/Ovis1.5-Llama3-8B -A flash_attention_2"  # test ✅ pass, time: 5.6s, mem: 19.3GB, 13/13 tests passed, 32T/1.4239351749420166s, 22.5 T/s
#CLI_COMMAND="python vision.py -m BAAI/Aquila-VL-2B-llava-qwen -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 24.7s, mem: 10.0GB, 13/13 tests passed, 27T/7.559697866439819s, 3.6 T/s
#CLI_COMMAND="python vision.py -m BAAI/Aquila-VL-2B-llava-qwen -A flash_attention_2"  # test ✅ pass, time: 9.5s, mem: 11.1GB, 13/13 tests passed, 27T/2.7635650634765625s, 9.8 T/s
#CLI_COMMAND="python vision.py -m BAAI/Emu2-Chat --load-in-4bit"  # test ✅ pass, time: 31.1s, mem: 28.8GB, 13/13 tests passed, 106T/11.065555810928345s, 9.6 T/s
#CLI_COMMAND="python vision.py -m CohereForAI/aya-vision-8b -A flash_attention_2"  # test ✅ pass, time: 33.1s, mem: 23.0GB, 13/13 tests passed, 218T/9.805668592453003s, 22.2 T/s
#CLI_COMMAND="python vision.py -m CohereForAI/aya-vision-32b -A flash_attention_2"  # test ✅ pass, time: 78.9s, mem: 67.1GB, 13/13 tests passed, 327T/25.267974376678467s, 12.9 T/s
#CLI_COMMAND="python vision.py -m HuggingFaceTB/SmolVLM-Instruct -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 7.6s, mem: 4.9GB, 13/13 tests passed, 33T/2.029047727584839s, 16.3 T/s
#CLI_COMMAND="python vision.py -m HuggingFaceTB/SmolVLM-Instruct -A flash_attention_2"  # test ✅ pass, time: 7.0s, mem: 7.7GB, 13/13 tests passed, 33T/1.845468521118164s, 17.9 T/s
#CLI_COMMAND="python vision.py -m HuggingFaceM4/Idefics3-8B-Llama3 -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 10.5s, mem: 8.1GB, 13/13 tests passed, 28T/2.922053813934326s, 9.6 T/s
#CLI_COMMAND="python vision.py -m HuggingFaceM4/Idefics3-8B-Llama3 -A flash_attention_2"  # test ✅ pass, time: 9.3s, mem: 19.0GB, 13/13 tests passed, 24T/2.469125509262085s, 9.7 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL-Chat-V1-5 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 22.2s, mem: 26.8GB, 13/13 tests passed, 60T/6.845478296279907s, 8.8 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL-Chat-V1-5 --device-map cuda:0 --max-tiles 40 --load-in-4bit"  # test ✅ pass, time: 29.0s, mem: 29.6GB, 13/13 tests passed, 58T/9.199079990386963s, 6.3 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL-Chat-V1-5 --device-map cuda:0 --max-tiles 40"  # test ✅ pass, time: 25.4s, mem: 54.5GB, 13/13 tests passed, 45T/8.031540632247925s, 5.6 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL-Chat-V1-5 --device-map cuda:0"  # test ✅ pass, time: 19.0s, mem: 51.9GB, 13/13 tests passed, 50T/5.906569480895996s, 8.5 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-1B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 9.7s, mem: 2.4GB, 13/13 tests passed, 68T/2.687544584274292s, 25.3 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-1B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 6.3s, mem: 3.1GB, 13/13 tests passed, 47T/1.6842317581176758s, 27.9 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-2B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 15.1s, mem: 6.5GB, 13/13 tests passed, 131T/4.6333277225494385s, 28.3 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-2B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 8.2s, mem: 8.4GB, 13/13 tests passed, 73T/2.36006498336792s, 30.9 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-4B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 11.3s, mem: 4.3GB, 13/13 tests passed, 57T/3.372122287750244s, 16.9 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-4B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 11.3s, mem: 8.2GB, 13/13 tests passed, 75T/3.394659996032715s, 22.1 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-8B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 11.3s, mem: 11.0GB, 13/13 tests passed, 48T/3.379678964614868s, 14.2 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-8B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 9.2s, mem: 20.1GB, 13/13 tests passed, 42T/2.6938209533691406s, 15.6 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-26B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 28.9s, mem: 30.6GB, 13/13 tests passed, 51T/9.11379623413086s, 5.6 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-26B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 27.0s, mem: 55.5GB, 13/13 tests passed, 52T/8.57758092880249s, 6.1 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-38B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 41.2s, mem: 33.0GB, 13/13 tests passed, 67T/13.017541885375977s, 5.1 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-38B --device-map cuda:0 --max-tiles 12"  # test ✅ pass, time: 34.1s, mem: 74.2GB, 13/13 tests passed, 57T/10.96051812171936s, 5.2 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2_5-78B --device-map cuda:0 --max-tiles 12 --load-in-4bit"  # test ✅ pass, time: 54.0s, mem: 56.7GB, 13/13 tests passed, 53T/17.383382320404053s, 3.0 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-1B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 25.4s, mem: 1.7GB, 13/13 tests passed, 271T/7.97890043258667s, 34.0 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-1B --device-map cuda:0"  # test ✅ pass, time: 7.7s, mem: 2.4GB, 13/13 tests passed, 77T/2.117676258087158s, 36.4 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-2B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 15.7s, mem: 4.5GB, 13/13 tests passed, 156T/4.709539890289307s, 33.1 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-2B --device-map cuda:0"  # test ✅ pass, time: 8.5s, mem: 6.4GB, 13/13 tests passed, 90T/2.4622414112091064s, 36.6 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-8B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 8.9s, mem: 8.3GB, 13/13 tests passed, 43T/2.6018552780151367s, 16.5 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-8B --device-map cuda:0"  # test ✅ pass, time: 7.8s, mem: 18.1GB, 13/13 tests passed, 43T/2.246669292449951s, 19.1 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-26B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 24.9s, mem: 26.5GB, 13/13 tests passed, 75T/7.898184061050415s, 9.5 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-26B --device-map cuda:0"  # test ✅ pass, time: 19.9s, mem: 51.8GB, 13/13 tests passed, 59T/6.310160398483276s, 9.4 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-40B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 34.3s, mem: 31.5GB, 13/13 tests passed, 82T/10.833274602890015s, 7.6 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-40B --device-map cuda:0"  # test ✅ pass, time: 43.7s, mem: 76.2GB, 13/13 tests passed, 140T/14.10234522819519s, 9.9 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/InternVL2-Llama3-76B --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 37.3s, mem: 54.0GB, 13/13 tests passed, 40T/11.808477640151978s, 3.4 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/Mini-InternVL-Chat-2B-V1-5 --load-in-4bit"  # test ✅ pass, time: 6.1s, mem: 5.1GB, 13/13 tests passed, 42T/1.6593635082244873s, 25.3 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/Mini-InternVL-Chat-2B-V1-5 --max-tiles 40 --load-in-4bit"  # test ✅ pass, time: 6.9s, mem: 6.8GB, 13/13 tests passed, 42T/1.8932747840881348s, 22.2 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/Mini-InternVL-Chat-2B-V1-5 --max-tiles 40"  # test ✅ pass, time: 6.8s, mem: 8.7GB, 13/13 tests passed, 48T/1.8108901977539062s, 26.5 T/s
#CLI_COMMAND="python vision.py -m OpenGVLab/Mini-InternVL-Chat-2B-V1-5"  # test ✅ pass, time: 5.8s, mem: 7.0GB, 13/13 tests passed, 48T/1.5911571979522705s, 30.2 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen-VL-Chat --load-in-4bit"  # test ✅ pass, time: 8.6s, mem: 12.0GB, 13/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen-VL-Chat"  # test ✅ pass, time: 6.4s, mem: 19.5GB, 13/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2-VL-2B-Instruct-AWQ -A flash_attention_2"  # test ✅ pass, time: 16.4s, mem: 13.4GB, 13/13 tests passed, 44T/4.455279588699341s, 9.9 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2-VL-2B-Instruct -A flash_attention_2"  # test ✅ pass, time: 21.3s, mem: 37.7GB, 13/13 tests passed, 36T/6.689767599105835s, 5.4 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2-VL-7B-Instruct-AWQ -A flash_attention_2"  # test ✅ pass, time: 25.3s, mem: 40.2GB, 13/13 tests passed, 36T/8.008622169494629s, 4.5 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2-VL-7B-Instruct -A flash_attention_2"  # test ✅ pass, time: 22.6s, mem: 48.9GB, 13/13 tests passed, 31T/7.1409406661987305s, 4.3 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2-VL-72B-Instruct-AWQ -A flash_attention_2"  # test ✅ pass, time: 43.2s, mem: 51.1GB, 13/13 tests passed, 31T/13.538523435592651s, 2.3 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-3B-Instruct-AWQ -F"  # test ✅ pass, time: 44.7s, mem: 37.0GB, 13/13 tests passed, 141T/14.245113372802734s, 9.9 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-3B-Instruct -F"  # test ✅ pass, time: 34.1s, mem: 40.7GB, 13/13 tests passed, 127T/10.918590068817139s, 11.6 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-7B-Instruct-AWQ -F"  # test ✅ pass, time: 46.9s, mem: 40.1GB, 13/13 tests passed, 185T/15.174686908721924s, 12.2 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-7B-Instruct -F"  # test ✅ pass, time: 37.6s, mem: 49.0GB, 13/13 tests passed, 198T/12.116867780685425s, 16.3 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-72B-Instruct -4F"  # test ✅ pass, time: 97.5s, mem: 78.6GB, 13/13 tests passed, 170T/31.529996871948242s, 5.4 T/s
#CLI_COMMAND="python vision.py -m Qwen/Qwen2.5-VL-72B-Instruct-AWQ -F"  # test ❌ fail, time: -1.0s, mem: -1.0GB, Error: Server failed to start (exit)., 0T/0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m Qwen/QVQ-72B-Preview -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 553.9s, mem: 78.6GB, 13/13 tests passed, 1591T/184.0021653175354s, 8.6 T/s
#CLI_COMMAND="python vision.py -m kosbu/QVQ-72B-Preview-AWQ -A flash_attention_2"  # test ✅ pass, time: 1032.5s, mem: 73.8GB, 13/13 tests passed, 1683T/344.186105966568s, 4.9 T/s
#CLI_COMMAND="python vision.py -m Salesforce/xgen-mm-phi3-mini-instruct-dpo-r-v1.5"  # test ✅ pass, time: 7.7s, mem: 9.5GB, 13/13 tests passed, 68T/2.1148743629455566s, 32.2 T/s
#CLI_COMMAND="python vision.py -m Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5"  # test ✅ pass, time: 3.4s, mem: 9.4GB, 13/13 tests passed, 10T/0.7093405723571777s, 14.1 T/s
#CLI_COMMAND="python vision.py -m Salesforce/xgen-mm-phi3-mini-instruct-singleimg-r-v1.5"  # test ✅ pass, time: 8.0s, mem: 9.5GB, 13/13 tests passed, 75T/2.254091739654541s, 33.3 T/s
#CLI_COMMAND="python vision.py -m Salesforce/xgen-mm-phi3-mini-instruct-r-v1"  # test ✅ pass, time: 8.6s, mem: 9.9GB, 13/13 tests passed, 82T/2.487712860107422s, 33.0 T/s
#CLI_COMMAND="python vision.py -m adept/fuyu-8b --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 20.1s, mem: 13.5GB, 13/13 tests passed, 92T/6.364359617233276s, 14.5 T/s
#CLI_COMMAND="python vision.py -m adept/fuyu-8b --device-map cuda:0"  # test ✅ pass, time: 16.5s, mem: 25.1GB, 13/13 tests passed, 79T/5.096308708190918s, 15.5 T/s
#CLI_COMMAND="python vision.py -m fancyfeast/joy-caption-alpha-two --load-in-4bit -A flash_attention_2"  # test ✅ pass, time: 58.5s, mem: 10.4GB, 13/13 tests passed, 246T/19.041832447052002s, 12.9 T/s
#CLI_COMMAND="python vision.py -m fancyfeast/joy-caption-alpha-two -A flash_attention_2"  # test ✅ pass, time: 34.8s, mem: 18.8GB, 13/13 tests passed, 214T/11.228965997695923s, 19.1 T/s
#CLI_COMMAND="python vision.py -m fancyfeast/joy-caption-pre-alpha -A flash_attention_2"  # test ✅ pass, time: 61.4s, mem: 18.1GB, 13/13 tests passed, 739T/21.025898456573486s, 35.1 T/s
#CLI_COMMAND="python vision.py -m internlm/internlm-xcomposer2d5-7b -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ❌ fail, time: 2.7s, mem: 9.0GB, 1/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m internlm/internlm-xcomposer2d5-7b -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 22.1s, mem: 28.3GB, 13/13 tests passed, 39T/7.169646263122559s, 5.4 T/s
#CLI_COMMAND="python vision.py -m internlm/internlm-xcomposer2-4khd-7b -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 21.8s, mem: 21.5GB, 13/13 tests passed, 46T/7.030832290649414s, 6.5 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-1.5-13b-hf -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 31.0s, mem: 9.1GB, 13/13 tests passed, 58T/4.453690767288208s, 13.0 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-1.5-13b-hf -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 9.2s, mem: 26.7GB, 13/13 tests passed, 59T/2.5110716819763184s, 23.5 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-1.5-7b-hf -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 9.5s, mem: 5.4GB, 13/13 tests passed, 62T/2.7935805320739746s, 22.2 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-1.5-7b-hf -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 7.5s, mem: 14.3GB, 13/13 tests passed, 65T/2.120906114578247s, 30.6 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-34b-hf -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 51.7s, mem: 22.4GB, 13/13 tests passed, 186T/16.641860723495483s, 11.2 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-34b-hf -A flash_attention_2"  # test ✅ pass, time: 72.7s, mem: 70.3GB, 13/13 tests passed, 246T/23.648174047470093s, 10.4 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-vicuna-13b-hf -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 14.9s, mem: 12.0GB, 13/13 tests passed, 55T/4.378801345825195s, 12.6 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-vicuna-13b-hf -A flash_attention_2"  # test ✅ pass, time: 13.5s, mem: 29.5GB, 13/13 tests passed, 55T/4.053428649902344s, 13.6 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-vicuna-7b-hf -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 14.4s, mem: 7.6GB, 13/13 tests passed, 88T/4.362282991409302s, 20.2 T/s
#CLI_COMMAND="python vision.py -m llava-hf/llava-v1.6-vicuna-7b-hf -A flash_attention_2"  # test ✅ pass, time: 12.4s, mem: 16.5GB, 13/13 tests passed, 98T/3.6723744869232178s, 26.7 T/s
#CLI_COMMAND="python vision.py -m lmms-lab/llava-onevision-qwen2-0.5b-ov -A flash_attention_2"  # test ✅ pass, time: 8.4s, mem: 9.9GB, 13/13 tests passed, 37T/2.2473063468933105s, 16.5 T/s
#CLI_COMMAND="python vision.py -m lmms-lab/llava-onevision-qwen2-7b-ov -A flash_attention_2"  # test ✅ pass, time: 17.5s, mem: 22.9GB, 13/13 tests passed, 51T/5.324223518371582s, 9.6 T/s
#CLI_COMMAND="python vision.py -m meta-llama/Llama-3.2-11B-Vision-Instruct -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 24.5s, mem: 10.3GB, 13/13 tests passed, 128T/7.149049520492554s, 17.9 T/s
#CLI_COMMAND="python vision.py -m meta-llama/Llama-3.2-11B-Vision-Instruct -A flash_attention_2"  # test ✅ pass, time: 24.7s, mem: 23.1GB, 13/13 tests passed, 254T/11.636772155761719s, 21.8 T/s
#CLI_COMMAND="python vision.py -m meta-llama/Llama-3.2-90B-Vision-Instruct -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 137.6s, mem: 53.5GB, 13/13 tests passed, 239T/42.01339912414551s, 5.7 T/s
#CLI_COMMAND="python vision.py -m microsoft/Florence-2-base-ft -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 2.5s, mem: 1.0GB, 13/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m microsoft/Florence-2-base-ft -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 2.1s, mem: 1.4GB, 13/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m mistralai/Pixtral-12B-2409"  # test ✅ pass, time: 19.5s, mem: 36.0GB, 13/13 tests passed, 0T/0.0s, 0.0 T/s
#CLI_COMMAND="python vision.py -m mx262/MiniMonkey -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 7.8s, mem: 7.7GB, 13/13 tests passed, 37T/2.229883909225464s, 16.6 T/s
#CLI_COMMAND="python vision.py -m mx262/MiniMonkey -A flash_attention_2"  # test ✅ pass, time: 7.2s, mem: 9.7GB, 13/13 tests passed, 37T/2.029930830001831s, 18.2 T/s
#CLI_COMMAND="python vision.py -m openbmb/MiniCPM-V-2_6 -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 13.9s, mem: 9.6GB, 13/13 tests passed, 102T/4.051717042922974s, 25.2 T/s
#CLI_COMMAND="python vision.py -m openbmb/MiniCPM-V-2_6 -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 11.8s, mem: 19.0GB, 13/13 tests passed, 91T/3.0875165462493896s, 29.5 T/s
#CLI_COMMAND="python vision.py -m openbmb/MiniCPM-Llama3-V-2_5 -A flash_attention_2 --device-map cuda:0 --load-in-4bit"  # test ✅ pass, time: 25.9s, mem: 8.9GB, 13/13 tests passed, 64T/7.820725202560425s, 8.2 T/s
#CLI_COMMAND="python vision.py -m openbmb/MiniCPM-Llama3-V-2_5 -A flash_attention_2 --device-map cuda:0"  # test ✅ pass, time: 26.3s, mem: 19.3GB, 13/13 tests passed, 76T/7.400312900543213s, 10.3 T/s
#CLI_COMMAND="python vision.py -m qresearch/llama-3-vision-alpha-hf --device cuda:0 --load-in-4bit"  # test ✅ pass, time: 10.9s, mem: 7.7GB, 13/13 tests passed, 68T/2.9753613471984863s, 22.9 T/s
#CLI_COMMAND="python vision.py -m qresearch/llama-3-vision-alpha-hf --device cuda:0"  # test ✅ pass, time: 8.3s, mem: 16.8GB, 13/13 tests passed, 77T/2.4877986907958984s, 31.0 T/s
#CLI_COMMAND="python vision.py -m vikhyatk/moondream2 -A flash_attention_2 --load-in-4bit"  # test ✅ pass, time: 5.1s, mem: 3.1GB, 13/13 tests passed, 63T/1.4403131008148193s, 43.7 T/s
#CLI_COMMAND="python vision.py -m vikhyatk/moondream2 -A flash_attention_2"  # test ✅ pass, time: 4.3s, mem: 4.8GB, 13/13 tests passed, 63T/1.1701526641845703s, 53.8 T/s
