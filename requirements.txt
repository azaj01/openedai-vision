accelerate
# AWQ and GPTQ support need to be installed separately
bitsandbytes==0.44.1
datasets
fastapi
# See: https://github.com/bdashore3/flash-attention/releases for other windows flash_attn releases
# And: https://github.com/Dao-AILab/flash-attention/releases for linux.
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.10"
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.11"
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.12"
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.13"
https://github.com/kingbri1/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.5.0cxx11abiFALSE-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
https://github.com/kingbri1/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.5.0cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://github.com/kingbri1/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.5.0cxx11abiFALSE-cp312-cp312-win_amd64.whl; platform_system == "Windows" and python_version == "3.12"
https://github.com/kingbri1/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu124torch2.5.0cxx11abiFALSE-cp313-cp313-win_amd64.whl; platform_system == "Windows" and python_version == "3.13"
flash_attn; python_version != "3.10" and python_version != "3.11" and python_version != "3.12" and python_version != "3.13"
hf_transfer
loguru
numpy
openai
peft
protobuf
pydantic
python-datauri
quanto
requests
sentencepiece
sse_starlette
torch==2.5.*
uvicorn
wandb
xformers

# moondream
deepspeed
einops
einops-exts
httpx
markdown2[all]
open_clip_torch
shortuuid
timm
tokenizers
torchvision

# qwen
matplotlib
optimum
tiktoken
transformers_stream_generator
qwen-vl-utils[decord]

# 360vl
logger

# mistral
mistral_inference
mistral_common[opencv]

# got-ocr2
verovio

# Aria. needs to build a bunch and doesn't work without many extra packages
# BYOB, use it if you need it
#grouped_gemm
